{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:45.377493Z",
     "iopub.status.busy": "2025-06-28T14:23:45.376963Z",
     "iopub.status.idle": "2025-06-28T14:23:45.384208Z",
     "shell.execute_reply": "2025-06-28T14:23:45.383569Z",
     "shell.execute_reply.started": "2025-06-28T14:23:45.377467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define base directories\n",
    "BASE_DIR = \"/kaggle/working\" # Kaggle's default working directory\n",
    "DATA_INPUT_DIR = \"/kaggle/input/bangladeshi-hospitals-eye-dataset/Bangladeshi Hospitals Dataset\" # Adjust if your dataset name is different\n",
    "\n",
    "# Create project subdirectories\n",
    "os.makedirs(os.path.join(BASE_DIR, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'utils'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'results'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/trainA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/trainB'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/testA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/testB'), exist_ok=True)\n",
    "\n",
    "print(\"Directories created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:47.266655Z",
     "iopub.status.busy": "2025-06-28T14:23:47.265992Z",
     "iopub.status.idle": "2025-06-28T14:23:50.390597Z",
     "shell.execute_reply": "2025-06-28T14:23:50.389577Z",
     "shell.execute_reply.started": "2025-06-28T14:23:47.266629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "\n",
    "\n",
    "!pip install torch torchvision numpy matplotlib Pillow\n",
    "\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:52.297516Z",
     "iopub.status.busy": "2025-06-28T14:23:52.297222Z",
     "iopub.status.idle": "2025-06-28T14:23:52.303982Z",
     "shell.execute_reply": "2025-06-28T14:23:52.303119Z",
     "shell.execute_reply.started": "2025-06-28T14:23:52.297492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile utils/image_pool.py\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class ImagePool():\n",
    "    \"\"\"This class implements an image buffer that stores previously generated images.\n",
    "    This buffer allows us to update discriminators using a history of generated images\n",
    "    rather than only the ones produced by the latest generator.\n",
    "    This can help stabilize training.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size):\n",
    "        \"\"\"Initialize the ImagePool class\n",
    "        Parameters:\n",
    "            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be used\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:  # create an empty pool\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        \"\"\"Return an image from the pool.\n",
    "        Parameters:\n",
    "            images: the latest generated images from the generator\n",
    "        Returns images from the buffer.\n",
    "\n",
    "        By 50/100, the buffer will return previously generated images rather than newly generated ones.\n",
    "        \"\"\"\n",
    "        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            image = torch.unsqueeze(image.data, 0)\n",
    "            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:  # the buffer is full; randomly select an image from the buffer or the current image\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:  # by 50% chance, the buffer will return a previously generated image\n",
    "                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:  # by another 50% chance, do not add the current image to the buffer\n",
    "                    return_images.append(image)\n",
    "        return_images = torch.cat(return_images, 0)  # collect all the images and return\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:54.966967Z",
     "iopub.status.busy": "2025-06-28T14:23:54.966688Z",
     "iopub.status.idle": "2025-06-28T14:23:54.972530Z",
     "shell.execute_reply": "2025-06-28T14:23:54.971840Z",
     "shell.execute_reply.started": "2025-06-28T14:23:54.966947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile utils/transforms.py\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_transforms(image_size=(256, 256), domain='A'):\n",
    "    \"\"\"\n",
    "    Returns a composed set of transformations for CycleGAN images.\n",
    "    Includes stronger data augmentation for domain B (healthy images).\n",
    "    \"\"\"\n",
    "    if domain == 'B':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "def get_test_transforms(image_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Returns a composed set of transformations for testing images (no augmentation).\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "# Example Usage (optional, can be removed for cleaner notebook)\n",
    "if __name__ == '__main__':\n",
    "    train_transform_A = get_transforms(domain='A')\n",
    "    train_transform_B = get_transforms(domain='B')\n",
    "    test_transform = get_test_transforms()\n",
    "    print(\"Train transforms (A):\", train_transform_A)\n",
    "    print(\"Train transforms (B):\", train_transform_B)\n",
    "    print(\"Test transforms:\", test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:56.957666Z",
     "iopub.status.busy": "2025-06-28T14:23:56.956963Z",
     "iopub.status.idle": "2025-06-28T14:23:56.962977Z",
     "shell.execute_reply": "2025-06-28T14:23:56.962230Z",
     "shell.execute_reply.started": "2025-06-28T14:23:56.957639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile utils/dataset.py\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "class CycleGAN_Dataset(Dataset):\n",
    "    def __init__(self, root_A, root_B, transform=None, transform_B=None, max_images=None):\n",
    "        self.root_A = root_A\n",
    "        self.root_B = root_B\n",
    "        self.transform = transform\n",
    "        self.transform_B = transform_B if transform_B is not None else transform\n",
    "\n",
    "        files_A_all = [os.path.join(root_A, f) for f in os.listdir(root_A) if os.path.isfile(os.path.join(root_A, f))]\n",
    "        files_B_all = [os.path.join(root_B, f) for f in os.listdir(root_B) if os.path.isfile(os.path.join(root_B, f))]\n",
    "\n",
    "        # Shuffle and limit files if max_images is specified\n",
    "        if max_images is not None:\n",
    "            random.shuffle(files_A_all)\n",
    "            random.shuffle(files_B_all)\n",
    "            self.files_A = files_A_all[:min(len(files_A_all), max_images)]\n",
    "            self.files_B = files_B_all[:min(len(files_B_all), max_images)]\n",
    "        else:\n",
    "            self.files_A = files_A_all\n",
    "            self.files_B = files_B_all\n",
    "\n",
    "        self.len_A = len(self.files_A)\n",
    "        self.len_B = len(self.files_B)\n",
    "        self.length = max(self.len_A, self.len_B)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_A = Image.open(self.files_A[index % self.len_A]).convert(\"RGB\")\n",
    "        img_B = Image.open(self.files_B[index % self.len_B]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "        if self.transform_B:\n",
    "            img_B = self.transform_B(img_B)\n",
    "\n",
    "        return img_A, img_B\n",
    "\n",
    "# Example Usage (optional, can be removed for cleaner notebook)\n",
    "if __name__ == '__main__':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    dataset_limited = CycleGAN_Dataset(root_A='data/trainA', root_B='data/trainB', transform=transform, max_images=100)\n",
    "    print(f\"Dataset length (limited to 100 per domain): {len(dataset_limited)}\")\n",
    "    dataset_full = CycleGAN_Dataset(root_A='data/trainA', root_B='data/trainB', transform=transform)\n",
    "    print(f\"Dataset length (full): {len(dataset_full)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:59.187270Z",
     "iopub.status.busy": "2025-06-28T14:23:59.186980Z",
     "iopub.status.idle": "2025-06-28T14:23:59.193272Z",
     "shell.execute_reply": "2025-06-28T14:23:59.192372Z",
     "shell.execute_reply.started": "2025-06-28T14:23:59.187248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile models/generator_A2B.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim)\n",
    "\n",
    "    def build_conv_block(self, dim):\n",
    "        conv_block = []\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True), nn.InstanceNorm2d(dim), nn.ReLU(True)]\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True), nn.InstanceNorm2d(dim)]\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Resnet-based generator\"\"\"\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        if type(norm_layer) == nn.BatchNorm2d:\n",
    "            use_bias = False\n",
    "        else:\n",
    "            use_bias = True\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "# Example Usage (optional, can be removed for cleaner notebook)\n",
    "if __name__ == '__main__':\n",
    "    generator = Generator(input_nc=3, output_nc=3)\n",
    "    dummy_input = torch.randn(1, 3, 256, 256)\n",
    "    output = generator(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:24:01.597420Z",
     "iopub.status.busy": "2025-06-28T14:24:01.596846Z",
     "iopub.status.idle": "2025-06-28T14:24:01.603200Z",
     "shell.execute_reply": "2025-06-28T14:24:01.602431Z",
     "shell.execute_reply.started": "2025-06-28T14:24:01.597396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile models/generator_B2A.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim)\n",
    "\n",
    "    def build_conv_block(self, dim):\n",
    "        conv_block = []\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True), nn.InstanceNorm2d(dim), nn.ReLU(True)]\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True), nn.InstanceNorm2d(dim)]\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Resnet-based generator\"\"\"\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        if type(norm_layer) == nn.BatchNorm2d:\n",
    "            use_bias = False\n",
    "        else:\n",
    "            use_bias = True\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "# Example Usage (optional, can be removed for cleaner notebook)\n",
    "if __name__ == '__main__':\n",
    "    generator = Generator(input_nc=3, output_nc=3)\n",
    "    dummy_input = torch.randn(1, 3, 256, 256)\n",
    "    output = generator(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:24:04.085835Z",
     "iopub.status.busy": "2025-06-28T14:24:04.085569Z",
     "iopub.status.idle": "2025-06-28T14:24:04.090990Z",
     "shell.execute_reply": "2025-06-28T14:24:04.090388Z",
     "shell.execute_reply.started": "2025-06-28T14:24:04.085815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile models/discriminator_A.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        if type(norm_layer) == nn.BatchNorm2d:\n",
    "            use_bias = False\n",
    "        else:\n",
    "            use_bias = True\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "# Example Usage (optional, can be removed for cleaner notebook)\n",
    "if __name__ == '__main__':\n",
    "    discriminator = Discriminator(input_nc=3)\n",
    "    dummy_input = torch.randn(1, 3, 256, 256)\n",
    "    output = discriminator(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:24:06.125906Z",
     "iopub.status.busy": "2025-06-28T14:24:06.125374Z",
     "iopub.status.idle": "2025-06-28T14:24:06.130899Z",
     "shell.execute_reply": "2025-06-28T14:24:06.130092Z",
     "shell.execute_reply.started": "2025-06-28T14:24:06.125882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile models/discriminator_B.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        if type(norm_layer) == nn.BatchNorm2d:\n",
    "            use_bias = False\n",
    "        else:\n",
    "            use_bias = True\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "# Example Usage (optional, can be removed for cleaner notebook)\n",
    "if __name__ == '__main__':\n",
    "    discriminator = Discriminator(input_nc=3)\n",
    "    dummy_input = torch.randn(1, 3, 256, 256)\n",
    "    output = discriminator(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:24:08.277493Z",
     "iopub.status.busy": "2025-06-28T14:24:08.276967Z",
     "iopub.status.idle": "2025-06-28T14:24:34.186306Z",
     "shell.execute_reply": "2025-06-28T14:24:34.185482Z",
     "shell.execute_reply.started": "2025-06-28T14:24:08.277466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cell 10: Data Organization (CORRECTED)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Ensure all necessary directories exist\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/trainA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/trainB'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/valA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/valB'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/testA'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'data/testB'), exist_ok=True)\n",
    "\n",
    "# Move Healthy images to a temporary 'all_healthy' directory\n",
    "temp_all_healthy_dir = os.path.join(BASE_DIR, 'data/all_healthy')\n",
    "os.makedirs(temp_all_healthy_dir, exist_ok=True)\n",
    "healthy_source_dir = os.path.join(DATA_INPUT_DIR, 'Healthy')\n",
    "for f in os.listdir(healthy_source_dir):\n",
    "    if os.path.isfile(os.path.join(healthy_source_dir, f)):\n",
    "        shutil.copy(os.path.join(healthy_source_dir, f), os.path.join(temp_all_healthy_dir, f))\n",
    "print(\"Copied Healthy images to temporary all_healthy directory.\")\n",
    "\n",
    "# Collect Diseased images from all subfolders, grouped by class\n",
    "temp_all_diseased_dir = os.path.join(BASE_DIR, 'data/all_diseased')\n",
    "os.makedirs(temp_all_diseased_dir, exist_ok=True)\n",
    "diseased_dirs = [d for d in os.listdir(DATA_INPUT_DIR) if os.path.isdir(os.path.join(DATA_INPUT_DIR, d)) and d != 'Healthy']\n",
    "diseased_files_by_class = {}\n",
    "for d in diseased_dirs:\n",
    "    class_dir = os.path.join(DATA_INPUT_DIR, d)\n",
    "    files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "    diseased_files_by_class[d] = files\n",
    "    for f in files:\n",
    "        shutil.copy(f, os.path.join(temp_all_diseased_dir, os.path.basename(f)))\n",
    "print(\"Copied Diseased images to temporary all_diseased directory.\")\n",
    "\n",
    "# Balance diseased images across classes (9 classes, target 2676 images)\n",
    "total_diseased_target = 2676  # Match healthy images\n",
    "num_classes = len(diseased_dirs)  # Should be 9\n",
    "images_per_class = total_diseased_target // num_classes  # ~297 images per class\n",
    "remainder = total_diseased_target % num_classes  # 3 extra images\n",
    "selected_diseased_files = []\n",
    "for class_name, files in diseased_files_by_class.items():\n",
    "    random.shuffle(files)\n",
    "    num_to_select = images_per_class + (1 if remainder > 0 else 0)\n",
    "    remainder -= 1 if remainder > 0 else 0\n",
    "    selected_diseased_files.extend(files[:min(len(files), num_to_select)])\n",
    "    print(f\"Selected {min(len(files), num_to_select)} images from {class_name}\")\n",
    "\n",
    "# Move selected diseased files back to temp_all_diseased_dir\n",
    "shutil.rmtree(temp_all_diseased_dir)\n",
    "os.makedirs(temp_all_diseased_dir, exist_ok=True)\n",
    "for f in selected_diseased_files:\n",
    "    shutil.copy(f, os.path.join(temp_all_diseased_dir, os.path.basename(f)))\n",
    "print(f\"Total selected diseased images: {len(selected_diseased_files)}\")\n",
    "\n",
    "# Split data into train, val, test\n",
    "def split_dataset(source_dir, train_dir, val_dir, test_dir, val_ratio=0.2, test_ratio=0.2):\n",
    "    all_files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
    "    random.shuffle(all_files)\n",
    "    num_total = len(all_files)\n",
    "    num_test = int(num_total * test_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_train = num_total - num_test - num_val\n",
    "\n",
    "    if num_train == 0 and num_total > 0: num_train = 1\n",
    "    if num_val == 0 and num_total > 1: num_val = 1\n",
    "    if num_test == 0 and num_total > 2: num_test = 1\n",
    "    if num_train + num_val + num_test > num_total:\n",
    "        diff = (num_train + num_val + num_test) - num_total\n",
    "        if num_train > diff: num_train -= diff\n",
    "        elif num_val > diff: num_val -= diff\n",
    "        elif num_test > diff: num_test -= diff\n",
    "\n",
    "    train_files = all_files[:num_train]\n",
    "    val_files = all_files[num_train:num_train + num_val]\n",
    "    test_files = all_files[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "    print(f\"Total files: {num_total}, Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "    for f in train_files:\n",
    "        shutil.move(os.path.join(source_dir, f), os.path.join(train_dir, f))\n",
    "    for f in val_files:\n",
    "        shutil.move(os.path.join(source_dir, f), os.path.join(val_dir, f))\n",
    "    for f in test_files:\n",
    "        shutil.move(os.path.join(source_dir, f), os.path.join(test_dir, f))\n",
    "\n",
    "    print(f\"Split data from {source_dir} into {train_dir}, {val_dir}, {test_dir}\")\n",
    "\n",
    "print(\"\\nSplitting Diseased data into trainA, valA, testA...\")\n",
    "split_dataset(temp_all_diseased_dir, \n",
    "              os.path.join(BASE_DIR, 'data/trainA'), \n",
    "              os.path.join(BASE_DIR, 'data/valA'), \n",
    "              os.path.join(BASE_DIR, 'data/testA'))\n",
    "\n",
    "print(\"\\nSplitting Healthy data into trainB, valB, testB...\")\n",
    "split_dataset(temp_all_healthy_dir, \n",
    "              os.path.join(BASE_DIR, 'data/trainB'), \n",
    "              os.path.join(BASE_DIR, 'data/valB'), \n",
    "              os.path.join(BASE_DIR, 'data/testB'))\n",
    "\n",
    "# Clean up temporary directories\n",
    "shutil.rmtree(temp_all_diseased_dir)\n",
    "shutil.rmtree(temp_all_healthy_dir)\n",
    "print(\"\\nTemporary data directories cleaned up.\")\n",
    "print(\"Final data organization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:38:10.190213Z",
     "iopub.status.busy": "2025-06-28T14:38:10.189443Z",
     "iopub.status.idle": "2025-06-28T14:38:10.200049Z",
     "shell.execute_reply": "2025-06-28T14:38:10.199373Z",
     "shell.execute_reply.started": "2025-06-28T14:38:10.190182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import modules from the current working directory\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/utils')\n",
    "sys.path.append('/kaggle/working/models')\n",
    "\n",
    "from dataset import CycleGAN_Dataset\n",
    "from transforms import get_transforms\n",
    "from image_pool import ImagePool\n",
    "from generator_A2B import Generator as Generator_A2B\n",
    "from generator_B2A import Generator as Generator_B2A\n",
    "from discriminator_A import Discriminator as Discriminator_A\n",
    "from discriminator_B import Discriminator as Discriminator_B\n",
    "\n",
    "# --- Hyperparameters and Configuration ---\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataroot_A = '/kaggle/working/data/trainA'\n",
    "        self.dataroot_B = '/kaggle/working/data/trainB'\n",
    "        self.val_dataroot_A = '/kaggle/working/data/valA'\n",
    "        self.val_dataroot_B = '/kaggle/working/data/valB'\n",
    "        self.test_dataroot_A = '/kaggle/working/data/testA'\n",
    "        self.test_dataroot_B = '/kaggle/working/data/testB'\n",
    "        self.batch_size = 16\n",
    "        self.image_size = 128  # Adjusted based on your preference\n",
    "        self.input_nc = 3\n",
    "        self.output_nc = 3\n",
    "        self.ngf = 64\n",
    "        self.ndf = 64\n",
    "        self.n_resnet_blocks = 9\n",
    "        self.lr_g = 0.0002  # Generator learning rate\n",
    "        self.lr_d = 0.0004  # Discriminator learning rate (TTUR)\n",
    "        self.beta1 = 0.5\n",
    "        self.num_epochs = 100  # Adjusted based on your preference\n",
    "        self.decay_epoch = 50\n",
    "        self.lambda_cycle = 10.0\n",
    "        self.lambda_identity = 1.0  # Reduced to encourage transformation\n",
    "        self.pool_size = 50\n",
    "        self.save_dir = '/kaggle/working/checkpoints'\n",
    "        self.results_dir = '/kaggle/working/results'\n",
    "        self.display_freq = 20\n",
    "        self.save_latest_freq = 100\n",
    "        self.save_epoch_freq = 5\n",
    "        self.early_stopping_patience = 20\n",
    "        self.early_stopping_min_delta = 0.01\n",
    "        self.max_train_images_per_domain = None  # Use all available images\n",
    "        self.max_val_images_per_domain = None\n",
    "\n",
    "opt = Config()\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Initialize Models ---\n",
    "netG_A2B = Generator_A2B(opt.input_nc, opt.output_nc, opt.ngf, n_blocks=opt.n_resnet_blocks)\n",
    "netG_B2A = Generator_B2A(opt.input_nc, opt.output_nc, opt.ngf, n_blocks=opt.n_resnet_blocks)\n",
    "netD_A = Discriminator_A(opt.input_nc, opt.ndf)\n",
    "netD_B = Discriminator_B(opt.input_nc, opt.ndf)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    netG_A2B = nn.DataParallel(netG_A2B)\n",
    "    netG_B2A = nn.DataParallel(netG_B2A)\n",
    "    netD_A = nn.DataParallel(netD_A)\n",
    "    netD_B = nn.DataParallel(netD_B)\n",
    "\n",
    "netG_A2B.to(device)\n",
    "netG_B2A.to(device)\n",
    "netD_A.to(device)\n",
    "netD_B.to(device)\n",
    "\n",
    "# --- Loss Functions ---\n",
    "criterionGAN = nn.MSELoss()\n",
    "criterionCycle = nn.L1Loss()\n",
    "criterionIdentity = nn.L1Loss()\n",
    "\n",
    "# --- Optimizers ---\n",
    "optimizer_G = optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=opt.lr_g, betas=(opt.beta1, 0.999))\n",
    "optimizer_D_A = optim.Adam(netD_A.parameters(), lr=opt.lr_d, betas=(opt.beta1, 0.999))\n",
    "optimizer_D_B = optim.Adam(netD_B.parameters(), lr=opt.lr_d, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# --- Learning Rate Schedulers ---\n",
    "def get_scheduler(optimizer, opt):\n",
    "    def lambda_rule(epoch):\n",
    "        lr_l = 1.0 - max(0, epoch + 1 - opt.decay_epoch) / (opt.num_epochs - opt.decay_epoch)\n",
    "        return lr_l\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    return scheduler\n",
    "\n",
    "scheduler_G = get_scheduler(optimizer_G, opt)\n",
    "scheduler_D_A = get_scheduler(optimizer_D_A, opt)\n",
    "scheduler_D_B = get_scheduler(optimizer_D_B, opt)\n",
    "\n",
    "# --- Image Pool for Discriminator Training ---\n",
    "fake_A_pool = ImagePool(opt.pool_size)\n",
    "fake_B_pool = ImagePool(opt.pool_size)\n",
    "\n",
    "# --- Data Loaders ---\n",
    "transform_A = get_transforms(image_size=(opt.image_size, opt.image_size), domain='A')\n",
    "transform_B = get_transforms(image_size=(opt.image_size, opt.image_size), domain='B')\n",
    "train_dataset = CycleGAN_Dataset(root_A=opt.dataroot_A, root_B=opt.dataroot_B, transform=transform_A, transform_B=transform_B)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "val_dataset = CycleGAN_Dataset(root_A=opt.val_dataroot_A, root_B=opt.val_dataroot_B, transform=transform_A, transform_B=transform_B)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=opt.batch_size, shuffle=False, num_workers=os.cpu_count() // 2 if os.cpu_count() else 0)\n",
    "\n",
    "# --- Early Stopping ---\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# --- Loss Tracking ---\n",
    "train_losses_G, val_losses_G = [], []\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(opt.num_epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    netG_A2B.train()\n",
    "    netG_B2A.train()\n",
    "    netD_A.train()\n",
    "    netD_B.train()\n",
    "\n",
    "    for i, (real_A, real_B) in enumerate(train_dataloader):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # --- Train Generators G_A2B and G_B2A ---\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        identity_B = netG_A2B(real_B)\n",
    "        loss_identity_B = criterionIdentity(identity_B, real_B) * opt.lambda_identity\n",
    "        identity_A = netG_B2A(real_A)\n",
    "        loss_identity_A = criterionIdentity(identity_A, real_A) * opt.lambda_identity\n",
    "\n",
    "        # GAN loss D_A(G_A2B(A))\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake_B = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterionGAN(pred_fake_B, torch.ones_like(pred_fake_B))\n",
    "\n",
    "        # GAN loss D_B(G_B2A(B))\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        pred_fake_A = netD_A(fake_A)\n",
    "        loss_GAN_B2A = criterionGAN(pred_fake_A, torch.ones_like(pred_fake_A))\n",
    "\n",
    "        # Cycle consistency loss\n",
    "        reconstructed_A = netG_B2A(fake_B)\n",
    "        loss_cycle_A = criterionCycle(reconstructed_A, real_A) * opt.lambda_cycle\n",
    "\n",
    "        reconstructed_B = netG_A2B(fake_A)\n",
    "        loss_cycle_B = criterionCycle(reconstructed_B, real_B) * opt.lambda_cycle\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_A + loss_cycle_B + loss_identity_A + loss_identity_B\n",
    "        loss_G.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), max_norm=1.0)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # --- Train Discriminator D_A ---\n",
    "        optimizer_D_A.zero_grad()\n",
    "        pred_real_A = netD_A(real_A)\n",
    "        loss_D_real_A = criterionGAN(pred_real_A, torch.ones_like(pred_real_A))\n",
    "        fake_A_from_pool = fake_A_pool.query(fake_A)\n",
    "        pred_fake_A = netD_A(fake_A_from_pool.detach())\n",
    "        loss_D_fake_A = criterionGAN(pred_fake_A, torch.zeros_like(pred_fake_A))\n",
    "        loss_D_A = (loss_D_real_A + loss_D_fake_A) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(netD_A.parameters(), max_norm=1.0)\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # --- Train Discriminator D_B ---\n",
    "        optimizer_D_B.zero_grad()\n",
    "        pred_real_B = netD_B(real_B)\n",
    "        loss_D_real_B = criterionGAN(pred_real_B, torch.ones_like(pred_real_B))\n",
    "        fake_B_from_pool = fake_B_pool.query(fake_B)\n",
    "        pred_fake_B = netD_B(fake_B_from_pool.detach())\n",
    "        loss_D_fake_B = criterionGAN(pred_fake_B, torch.zeros_like(pred_fake_B))\n",
    "        loss_D_B = (loss_D_real_B + loss_D_fake_B) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(netD_B.parameters(), max_norm=1.0)\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        # --- Print Training Progress ---\n",
    "        if i % opt.display_freq == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{opt.num_epochs}], Step [{i+1}/{len(train_dataloader)}], \"\n",
    "                  f\"Loss_G: {loss_G.item():.4f}, Loss_D_A: {loss_D_A.item():.4f}, Loss_D_B: {loss_D_B.item():.4f}, \"\n",
    "                  f\"Loss_GAN_A2B: {loss_GAN_A2B.item():.4f}, Loss_GAN_B2A: {loss_GAN_B2A.item():.4f}, \"\n",
    "                  f\"Loss_cycle_A: {loss_cycle_A.item():.4f}, Loss_cycle_B: {loss_cycle_B.item():.4f}, \"\n",
    "                  f\"Loss_identity_A: {loss_identity_A.item():.4f}, Loss_identity_B: {loss_identity_B.item():.4f}\")\n",
    "            \n",
    "            # Save generated images for visualization\n",
    "            if not os.path.exists(opt.results_dir):\n",
    "                os.makedirs(opt.results_dir)\n",
    "            save_image(fake_B.detach(), os.path.join(opt.results_dir, f'fake_B_epoch{epoch+1}_step{i+1}.png'), normalize=True)\n",
    "            save_image(fake_A.detach(), os.path.join(opt.results_dir, f'fake_A_epoch{epoch+1}_step{i+1}.png'), normalize=True)\n",
    "            save_image(real_A.detach(), os.path.join(opt.results_dir, f'real_A_epoch{epoch+1}_step{i+1}.png'), normalize=True)\n",
    "            save_image(real_B.detach(), os.path.join(opt.results_dir, f'real_B_epoch{epoch+1}_step{i+1}.png'), normalize=True)\n",
    "\n",
    "        # Save more frequent visualizations\n",
    "        if i % 10 == 0:\n",
    "            save_image(fake_B.detach(), os.path.join(opt.results_dir, f'fake_B_epoch{epoch+1}_step{i+1}.png'), normalize=True)\n",
    "            save_image(real_B.detach(), os.path.join(opt.results_dir, f'real_B_epoch{epoch+1}_step{i+1}.png'), normalize=True)\n",
    "\n",
    "        # Track training loss\n",
    "        train_losses_G.append(loss_G.item())\n",
    "\n",
    "    # Update learning rates\n",
    "    scheduler_G.step()\n",
    "    scheduler_D_A.step()\n",
    "    scheduler_D_B.step()\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    netG_A2B.eval()\n",
    "    netG_B2A.eval()\n",
    "    val_losses_G = []\n",
    "    with torch.no_grad():\n",
    "        for i, (real_A_val, real_B_val) in enumerate(val_dataloader):\n",
    "            real_A_val = real_A_val.to(device)\n",
    "            real_B_val = real_B_val.to(device)\n",
    "\n",
    "            # Calculate generator losses on validation set\n",
    "            identity_B_val = netG_A2B(real_B_val)\n",
    "            loss_identity_B_val = criterionIdentity(identity_B_val, real_B_val) * opt.lambda_identity\n",
    "            identity_A_val = netG_B2A(real_A_val)\n",
    "            loss_identity_A_val = criterionIdentity(identity_A_val, real_A_val) * opt.lambda_identity\n",
    "\n",
    "            fake_B_val = netG_A2B(real_A_val)\n",
    "            pred_fake_B_val = netD_B(fake_B_val)\n",
    "            loss_GAN_A2B_val = criterionGAN(pred_fake_B_val, torch.ones_like(pred_fake_B_val))\n",
    "\n",
    "            fake_A_val = netG_B2A(real_B_val)\n",
    "            pred_fake_A_val = netD_A(fake_A_val)\n",
    "            loss_GAN_B2A_val = criterionGAN(pred_fake_A_val, torch.ones_like(pred_fake_A_val))\n",
    "\n",
    "            reconstructed_A_val = netG_B2A(fake_B_val)\n",
    "            loss_cycle_A_val = criterionCycle(reconstructed_A_val, real_A_val) * opt.lambda_cycle\n",
    "\n",
    "            reconstructed_B_val = netG_A2B(fake_A_val)\n",
    "            loss_cycle_B_val = criterionCycle(reconstructed_B_val, real_B_val) * opt.lambda_cycle\n",
    "\n",
    "            total_val_loss_G = loss_GAN_A2B_val + loss_GAN_B2A_val + loss_cycle_A_val + loss_cycle_B_val + loss_identity_A_val + loss_identity_B_val\n",
    "            val_losses_G.append(total_val_loss_G.item())\n",
    "    \n",
    "    avg_val_loss_G = np.mean(val_losses_G)\n",
    "    val_losses_G.append(avg_val_loss_G)\n",
    "    print(f\"Epoch [{epoch+1}/{opt.num_epochs}], Average Validation Loss G: {avg_val_loss_G:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Logic ---\n",
    "    if avg_val_loss_G < best_val_loss - opt.early_stopping_min_delta:\n",
    "        best_val_loss = avg_val_loss_G\n",
    "        epochs_no_improve = 0\n",
    "        if not os.path.exists(opt.save_dir):\n",
    "            os.makedirs(opt.save_dir)\n",
    "        torch.save(netG_A2B.state_dict(), os.path.join(opt.save_dir, 'netG_A2B_best.pth'))\n",
    "        torch.save(netG_B2A.state_dict(), os.path.join(opt.save_dir, 'netG_B2A_best.pth'))\n",
    "        torch.save(netD_A.state_dict(), os.path.join(opt.save_dir, 'netD_A_best.pth'))\n",
    "        torch.save(netD_B.state_dict(), os.path.join(opt.save_dir, 'netD_B_best.pth'))\n",
    "        print(f\"Saved best models at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss did not improve for {epochs_no_improve} epochs.\")\n",
    "        if epochs_no_improve >= opt.early_stopping_patience:\n",
    "            early_stop = True\n",
    "\n",
    "    # Save models periodically\n",
    "    if (epoch + 1) % opt.save_epoch_freq == 0:\n",
    "        if not os.path.exists(opt.save_dir):\n",
    "            os.makedirs(opt.save_dir)\n",
    "        torch.save(netG_A2B.state_dict(), os.path.join(opt.save_dir, f'netG_A2B_epoch_{epoch+1}.pth'))\n",
    "        torch.save(netG_B2A.state_dict(), os.path.join(opt.save_dir, f'netG_B2A_epoch_{epoch+1}.pth'))\n",
    "        torch.save(netD_A.state_dict(), os.path.join(opt.save_dir, f'netD_A_epoch_{epoch+1}.pth'))\n",
    "        torch.save(netD_B.state_dict(), os.path.join(opt.save_dir, f'netD_B_epoch_{epoch+1}.pth'))\n",
    "        print(f\"Models saved for epoch {epoch+1}\")\n",
    "\n",
    "# Save loss plot\n",
    "plt.plot(train_losses_G, label='Train Loss G')\n",
    "plt.plot(val_losses_G, label='Val Loss G')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(opt.results_dir, 'loss_plot.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:38:15.001560Z",
     "iopub.status.busy": "2025-06-28T14:38:15.001280Z",
     "iopub.status.idle": "2025-06-28T19:03:04.392891Z",
     "shell.execute_reply": "2025-06-28T19:03:04.391893Z",
     "shell.execute_reply.started": "2025-06-28T14:38:15.001539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: Run Training\n",
    "\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile predict.py\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Import modules from the current working directory (Kaggle's /kaggle/working/)\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/models')\n",
    "sys.path.append('/kaggle/working/utils')\n",
    "\n",
    "from generator_A2B import Generator as Generator_A2B\n",
    "from transforms import get_test_transforms\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.image_size = 256\n",
    "        self.input_nc = 3\n",
    "        self.output_nc = 3\n",
    "        self.ngf = 64 # number of generator filters\n",
    "        self.n_resnet_blocks = 9\n",
    "        self.model_path = '/kaggle/working/checkpoints/netG_A2B_best.pth' # Path to your trained generator model (changed to best model)\n",
    "        self.input_image_path = '/kaggle/working/data/testA/sample_diseased_eye.jpg' # Path to the diseased eye image you want to translate\n",
    "        self.output_image_path = '/kaggle/working/results/predicted_healthy_eye.png' # Path to save the translated healthy eye image\n",
    "\n",
    "opt = Config()\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Initialize Generator ---\n",
    "netG_A2B = Generator_A2B(opt.input_nc, opt.output_nc, opt.ngf, n_blocks=opt.n_resnet_blocks).to(device)\n",
    "\n",
    "# Load trained model weights\n",
    "if os.path.exists(opt.model_path):\n",
    "    # Load the state_dict\n",
    "    state_dict = torch.load(opt.model_path, map_location=device)\n",
    "    \n",
    "    # Create a new ordered dict without 'module.' prefix\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[7:]] = v # remove 'module.' prefix\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "            \n",
    "    netG_A2B.load_state_dict(new_state_dict) # Load the modified state_dict\n",
    "    netG_A2B.eval() # Set generator to evaluation mode\n",
    "    print(f\"Loaded model from {opt.model_path}\")\n",
    "else:\n",
    "    print(f\"Error: Model not found at {opt.model_path}. Please train the model first.\")\n",
    "    exit()\n",
    "\n",
    "# --- Image Transformation ---\n",
    "transform = get_test_transforms(image_size=(opt.image_size, opt.image_size))\n",
    "\n",
    "# --- Load and Transform Input Image ---\n",
    "if os.path.exists(opt.input_image_path):\n",
    "    input_image = Image.open(opt.input_image_path).convert(\"RGB\")\n",
    "    input_tensor = transform(input_image).unsqueeze(0).to(device) # Add batch dimension\n",
    "    print(f\"Loaded input image from {opt.input_image_path}\")\n",
    "else:\n",
    "    print(f\"Error: Input image not found at {opt.input_image_path}. Please provide a valid image.\")\n",
    "    exit()\n",
    "\n",
    "# --- Perform Inference ---\n",
    "with torch.no_grad():\n",
    "    output_tensor = netG_A2B(input_tensor)\n",
    "\n",
    "# --- Save Output Image ---\n",
    "if not os.path.exists(os.path.dirname(opt.output_image_path)):\n",
    "    os.makedirs(os.path.dirname(opt.output_image_path))\n",
    "save_image(output_tensor, opt.output_image_path, normalize=True)\n",
    "print(f\"Translated image saved to {opt.output_image_path}\")\n",
    "\n",
    "print(\"Prediction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T19:13:27.147357Z",
     "iopub.status.busy": "2025-06-28T19:13:27.146221Z",
     "iopub.status.idle": "2025-06-28T19:13:27.190914Z",
     "shell.execute_reply": "2025-06-28T19:13:27.190211Z",
     "shell.execute_reply.started": "2025-06-28T19:13:27.147308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15: Move Best Model for Download\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "source_path = '/kaggle/working/checkpoints/netG_A2B_best.pth'\n",
    "destination_path = '/kaggle/working/netG_A2B_best.pth' # Move to the root of the working directory\n",
    "\n",
    "if os.path.exists(source_path):\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    print(f\"Successfully copied {source_path} to {destination_path}\")\n",
    "else:\n",
    "    print(f\"Error: Model not found at {source_path}. Please ensure training completed successfully.\")\n",
    "\n",
    "# You can also move other best models if needed, e.g.:\n",
    "# shutil.copy('/kaggle/working/checkpoints/netG_B2A_best.pth', '/kaggle/working/netG_B2A_best.pth')\n",
    "# shutil.copy('/kaggle/working/checkpoints/netD_A_best.pth', '/kaggle/working/netD_A_best.pth')\n",
    "# shutil.copy('/kaggle/working/checkpoints/netD_B_best.pth', '/kaggle/working/netD_B_best.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: Run Prediction\n",
    "\n",
    "# Before running this, ensure you have a sample image in /kaggle/working/data/testA/\n",
    "# For example, you might copy one from your dataset:\n",
    "# !cp /kaggle/input/bangladeshi-hospitals-eye-dataset/Bangladeshi\\ Hospitals\\ Dataset/Central\\ Serous\\ Chorioretinopathy\\ \\[Color\\ Fundus\\]/CSCR1.jpg /kaggle/working/data/testA/sample_diseased_eye.jpg\n",
    "\n",
    "!python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T14:23:27.538022Z",
     "iopub.status.busy": "2025-06-28T14:23:27.537169Z",
     "iopub.status.idle": "2025-06-28T14:23:27.847846Z",
     "shell.execute_reply": "2025-06-28T14:23:27.847288Z",
     "shell.execute_reply.started": "2025-06-28T14:23:27.537993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = '/kaggle/working/'\n",
    "\n",
    "# Remove all files and directories in /kaggle/working/\n",
    "for item in os.listdir(output_dir):\n",
    "    item_path = os.path.join(output_dir, item)\n",
    "    try:\n",
    "        if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "            os.unlink(item_path)  # Remove file or link\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)  # Remove directory and its contents\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {item_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7757120,
     "sourceId": 12306754,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 247762120,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
